# ==============================================================================
# MLX LoRA Configuration: Lovecraft Style Transfer (Final)
# Dataset: 8,600 Total / 6,880 Train
# Strategy: High-Rank, High-Dropout, Base Model
# ==============================================================================

model: "mlx-community/Qwen3-8B-Base-bf16"
train: true
data: "data/training/lovecraft_150"

# Base models work best with raw text completion
fine_tune_type: lora

# --- Training Strategy ---
# Effective Batch Size = 2 (1 * 2)
# We want "spiky" gradients to learn specific style quirks, not averaged smoothness.
batch_size: 1
grad_accumulation: 2

# Duration Logic:
# Train Set: 6,880 items
# 1 Epoch = 3,440 steps
# Target: ~0.87 Epochs (3,000 steps). 
# This sees 87% of data, preventing "catastrophic forgetting" of basic English.
iters: 3000

# Optimization
learning_rate: 1e-5

# Apply LoRA to ALL layers to capture deep structural reasoning
num_layers: -1

# --- LoRA Architecture ---
lora_parameters:
  # Rank 64: High capacity for complex Victorian sentence structures.
  rank: 64
  
  # Scale 2.0: Strong signal override.
  scale: 2.0
  
  # Dropout 0.15: The "NEFTune" simulation. 
  # Forces the model to ignore specific keywords and learn the structural rhythm.
  dropout: 0.15
  
  # Target Modules:
  # Full paths required for Qwen models.
  keys:
    - "self_attn.q_proj"
    - "self_attn.k_proj"
    - "self_attn.v_proj"
    - "self_attn.o_proj"
    - "mlp.gate_proj"
    - "mlp.up_proj"
    - "mlp.down_proj"    

# --- Checkpointing & Reporting ---
adapter_path: "lora_adapters/lovecraft"
save_every: 500       # Saves 6 checkpoints total
steps_per_report: 50  # Monitor loss frequently
steps_per_eval: 200   # Check against your 860 validation items

# Reproducibility
seed: 42
