# ==============================================================================
# MLX LoRA Configuration: Lovecraft Style Transfer (With Perspective Variations)
# Dataset: 8,840 Total / 7,072 Train / 884 Valid / 884 Test
# Strategy: High-Rank, High-Dropout, Base Model
# ==============================================================================

model: "./models/Qwen2.5-14B-Base-4bit-MLX"
train: true
data: "data/training/lovecraft"

# Base models work best with raw text completion
fine_tune_type: lora

# --- Training Strategy ---
# Effective Batch Size = 4 (1 * 4)
# We want "spiky" gradients to learn specific style quirks, not averaged smoothness.
batch_size: 1
grad_accumulation: 4
grad_checkpoint: true

# Duration Logic:
# Train Set: 7,072 items
# Effective batch: 4
# 1 Epoch = 7,072 / 4 = 1,768 steps
# Target: ~1.2 Epochs (2,100 steps) to learn perspective variations well
iters: 2100

# Optimization
learning_rate: 1e-5

# Apply LoRA to ALL layers to capture deep structural reasoning
num_layers: -1
#num_layers: 16 #if out of mem

# --- LoRA Architecture ---
lora_parameters:
  # Rank 64: High capacity for complex Victorian sentence structures.
  rank: 64
  
  # Scale 2.0: Strong signal override.
  scale: 2.0
  
  # Dropout 0.1: The "NEFTune" simulation. 
  # Forces the model to ignore specific keywords and learn the structural rhythm.
  dropout: 0.1
  
  # Target Modules:
  # Full paths required for Qwen models.    
  keys:
    - "self_attn.q_proj"
    - "self_attn.k_proj" # We skip k_proj to save a bit more, it's the least important for style.
    - "self_attn.v_proj"
    - "self_attn.o_proj"
    - "mlp.gate_proj"
    - "mlp.up_proj"
    - "mlp.down_proj"    

# --- Checkpointing & Reporting ---
adapter_path: "lora_adapters/lovecraft_14b"
save_every: 200       # Saves checkpoints every 200 steps
steps_per_report: 10  # Monitor loss frequently
steps_per_eval: 200   # Check against 884 validation items

# Reproducibility
seed: 42
